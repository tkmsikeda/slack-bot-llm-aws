# このアプリのデモ動画

https://github.com/user-attachments/assets/1f67d4a9-257b-4eac-956e-0840720c20a9

* SlackのボットとしてOpenAIのGPTモデルと会話できます。
* 回答途中の内容を随時更新しながら、表示することで、回答の待ち時間を短縮しています。
* 以上のシンプルなアプリです。

# このアプリの実現方法
## アーキテクチャと処理概要
![image](https://github.com/user-attachments/assets/51ef55ce-8bab-4b94-a6ae-f13d7e3d2c65)


* おおまかにはSlack、AWS、OpenAI APIの3構成です。
* アプリはAWS上で動作していて、SlackとOpenAI APIの繋ぎの役割です。
* LLMの回答はOpenAI APIを利用しています。
## 処理フローについて
1. 一次回答
    1. Slackにとりあえずの応答として、回答生成中の旨を送信します。
    2. Slackからのレスポンスに含まれているチャンネルIDとタイムスタンプを取得します。この情報をSQS経由で次のLambdaへ引き継ぎます。
2. 非同期で引き継ぎ
    1. SQSへチャンネルID、タイムスタンプ、ユーザからのクエリを登録します。
        1. チャンネルID、タイムスタンプはLLMの回答取得後に、「回答生成中」のメッセージを後から更新するために利用します。
        2. ユーザからのクエリは、LLMへ渡して、回答生成させるために利用します。        
4. 回答取得
    1. OpenAI APIへユーザクエリを渡して、Streamingを有効化してAPIをキックします。
    2. OpenAI APIからチャンク毎にレスポンスが返ってきます。
5. 回答送信
    1. OpenAI APIからチャンク毎に返ってくるレスポンスを0.5秒のインターバルでまとめて、Slack上に繰り返し反映します。チャンク毎に反映しないのは、チャンク毎だと早すぎて、Slack上の描画が逆に遅くなってしまうので、止む無く、まとめて更新しています。
